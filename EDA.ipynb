{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYGeK8TpYXPz"
      },
      "source": [
        "# **Exploratory Data Analysis**\n",
        "\n",
        "In this program, we will analyze comments of banking services by visualizing word frequency as wordcloud. Three wordclound for possitive, negative and neutral comments will be displayed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: pythainlp in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.1.4)\nRequirement already satisfied: tinydb>=3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pythainlp) (3.15.2)\nRequirement already satisfied: requests>=2.22.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pythainlp) (2.22.0)\nRequirement already satisfied: tqdm>=4.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pythainlp) (4.43.0)\nRequirement already satisfied: nltk>=3.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pythainlp) (3.4.5)\nRequirement already satisfied: dill>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pythainlp) (0.3.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (2019.9.11)\nRequirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (1.25.6)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (3.0.4)\nRequirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk>=3.3->pythainlp) (1.12.0)\n"
        }
      ],
      "source": [
        "!pip3 install pythainlp # install the pythainlp library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b8cb371f50ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "# Import python libraries used in this program\n",
        "import string\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.probability import FreqDist\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.util import normalize\n",
        "from pythainlp.tag import pos_tag\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "from pythainlp import thai_punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0liQPEQ2OeJS"
      },
      "outputs": [],
      "source": [
        "# Define proprocessing function \n",
        "def preprocessing(text):\n",
        "  text = unicodedata.normalize(\"NFKD\", text)\n",
        "  # step 1: word tokenization \n",
        "  token = word_tokenize(text, engine=\"longest\", keep_whitespace=False)\n",
        "  # step 2: word normalization \n",
        "  normalized_token = []\n",
        "  for item in token:\n",
        "    normalized_token.append(normalize(item))\n",
        "  #step 3: remove stop words\n",
        "  stopwords = thai_stopwords()\n",
        "  woStopword_token = []\n",
        "  for item in normalized_token:\n",
        "    if item not in stopwords:\n",
        "      woStopword_token.append(item)\n",
        "  #step 4: remove punctuation\n",
        "  en_punctuation = string.punctuation\n",
        "  th_punctuation = thai_punctuations\n",
        "  punctuation = en_punctuation+th_punctuation\n",
        "  final_token = []\n",
        "  for item in woStopword_token:\n",
        "\t  if item not in punctuation:\n",
        "\t\t  final_token.append(item) \n",
        "  return final_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e0YGNpLUd1Zm"
      },
      "outputs": [],
      "source": [
        "def filteringPOS(tokens,poslist):\n",
        "  pos = pos_tag(tokens,engine='perceptron',corpus='orchid')\n",
        "  filtered_token = []\n",
        "  for item in pos:\n",
        "    if item[1] in poslist:\n",
        "      filtered_token.append(item[0])\n",
        "  return filtered_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n3I0jM1qO7MD"
      },
      "outputs": [],
      "source": [
        "# Read data from an excel file (i.e. FB180_Social_Dataset_Classification.xlsx) \n",
        "data = pd.read_excel('FB180_Social_Dataset_Classification.xlsx') # You may need to change the file name. Please get the file name from the previous cell\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oeRHrqR8QvGV"
      },
      "outputs": [],
      "source": [
        "# Explore possitive comments\n",
        "# Step 1: select recodes of possitive comments from data\n",
        "possitive_comments = data[(data.Sentiment == 'positive')]\n",
        "possitive_comments.head()\n",
        "\n",
        "# Step 2: create a set of words extracted from possitive comments\n",
        "possitive_words = []\n",
        "for item in possitive_comments.itertuples():\n",
        "  token = preprocessing(item.Text)\n",
        "  #token = filteringPOS(token,[\"ADVN\",\"ADV\",\"ADVI\",\"ADVP\",\"ADVS\",\"ADJ\",\"NONM\",\"VATT\",\"DONM\",\"PART\",\"FIXN\",\"FIXV\",\"EAFF\",\"EITT\",\"AITT\",\"NEG\"])\n",
        "  possitive_words = possitive_words+token\n",
        "\n",
        "# Step 3: count word frequency\n",
        "fdist = FreqDist(possitive_words)\n",
        "\n",
        "# Step 4: visualize wordcloud\n",
        "wc = WordCloud(font_path='THSarabunNew.ttf',background_color=\"white\", regexp = r\"[ก-๙a-zA-Z']+\",)\n",
        "# generate word cloud\n",
        "wc.generate_from_frequencies(fdist)\n",
        "# show\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Cb5xrK7sXdqL"
      },
      "outputs": [],
      "source": [
        "# Explore negative comments\n",
        "# Step 1: select recodes of negative comments from data\n",
        "negative_comments = data[(data.Sentiment == 'negative')]\n",
        "negative_comments.head()\n",
        "\n",
        "# Step 2: create a set of words extracted from negative comments\n",
        "negative_words = []\n",
        "for item in negative_comments.itertuples():\n",
        "  token = preprocessing(item.Text)\n",
        "  #token = filteringPOS(token,[\"ADVN\",\"ADV\",\"ADVI\",\"ADVP\",\"ADVS\",\"ADJ\",\"NONM\",\"VATT\",\"DONM\",\"PART\",\"FIXN\",\"FIXV\",\"EAFF\",\"EITT\",\"AITT\",\"NEG\"])\n",
        "  negative_words = negative_words+token\n",
        "\n",
        "# Step 3: count word frequency\n",
        "fdist = FreqDist(negative_words)\n",
        "\n",
        "# Step 4: visualize wordcloud\n",
        "wc = WordCloud(font_path='THSarabunNew.ttf',background_color=\"white\", regexp = r\"[ก-๙a-zA-Z']+\",)\n",
        "# generate word cloud\n",
        "wc.generate_from_frequencies(fdist)\n",
        "# show\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AQYWu3bDYFn3"
      },
      "outputs": [],
      "source": [
        "# Explore neutral comments\n",
        "# Step 1: select recodes of neutral comments from data\n",
        "neutral_comments = data[(data.Sentiment == 'neutral')]\n",
        "neutral_comments.head()\n",
        "\n",
        "# Step 2: create a set of words extracted from neutral comments\n",
        "neutral_words = []\n",
        "for item in neutral_comments.itertuples():\n",
        "  token = preprocessing(item.Text)\n",
        "  #token = filteringPOS(token,[\"ADVN\",\"ADV\",\"ADVI\",\"ADVP\",\"ADVS\",\"ADJ\",\"NONM\",\"VATT\",\"DONM\",\"PART\",\"FIXN\",\"FIXV\",\"EAFF\",\"EITT\",\"AITT\",\"NEG\"])\n",
        "  neutral_words = neutral_words+token\n",
        "\n",
        "# Step 3: count word frequency\n",
        "fdist = FreqDist(neutral_words)\n",
        "\n",
        "# Step 4: visualize wordcloud\n",
        "wc = WordCloud(font_path='THSarabunNew.ttf',background_color=\"white\", regexp = r\"[ก-๙a-zA-Z']+\",)\n",
        "# generate word cloud\n",
        "wc.generate_from_frequencies(fdist)\n",
        "# show\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ]
}