{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_sWtCq03AbZ",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Analysis for Thai Texts**\n",
        "The process of sentiment analysis is given as follows:\n",
        "1. Load dataset into the program and split the dataset into training and test set\n",
        "2. Prepare training set: extract tf-idf and word2vec feature vectors from sentences\n",
        "3. Prepare test set: extract tf-idf and word2vec feature vectors from sentences\n",
        "4. Construct classifiers (i.e. SVM and kNN) using training set\n",
        "5. Evaluate the classiffiers using test set \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qVR2hnj2_4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pythainlp # install the pythainlp library"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOdHR7AV3RS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import python libraries used in this program\n",
        "import string\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.util import normalize\n",
        "from pythainlp.tag import pos_tag\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "from pythainlp import thai_punctuations\n",
        "import pythainlp.word_vector as thword2vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_dzNsud3WzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define proprocessing function \n",
        "def preprocessing(text):\n",
        "  text = unicodedata.normalize(\"NFKD\", text)\n",
        "  # step 1: word tokenization \n",
        "  token = word_tokenize(text, engine=\"longest\", keep_whitespace=False)\n",
        "  # step 2: word normalization \n",
        "  normalized_token = []\n",
        "  for item in token:\n",
        "    normalized_token.append(normalize(item))\n",
        "  #step 3: remove stop words\n",
        "  stopwords = thai_stopwords()\n",
        "  woStopword_token = []\n",
        "  for item in normalized_token:\n",
        "    if item not in stopwords:\n",
        "      woStopword_token.append(item)\n",
        "  #step 4: remove punctuation\n",
        "  en_punctuation = string.punctuation\n",
        "  th_punctuation = thai_punctuations\n",
        "  punctuation = en_punctuation+th_punctuation\n",
        "  final_token = []\n",
        "  for item in woStopword_token:\n",
        "\t  if item not in punctuation:\n",
        "\t\t  final_token.append(item) \n",
        "  return final_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EX2rJ_e-81d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence2vec(text):\n",
        "  model = thword2vec.get_model()\n",
        "  tokens = preprocessing(text)\n",
        "  xall = np.empty((0,300), float)\n",
        "  for item in tokens:\n",
        "    if item in model.index2word:\n",
        "      wordvec = model[item][np.newaxis]\n",
        "      xall = np.append(xall,wordvec,0)\n",
        "  x = np.mean(xall, axis=0)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV7I3S363ZPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: load dataset into program and split the dataset into random train and test subsets\n",
        "data = pd.read_excel('FB180_Social_Dataset_Classification.xlsx')\n",
        "x = data['Text']\n",
        "y = data['Sentiment']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUD8_6Fx32kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2: prepare training dataset - using tf-idf and word2vec as feature representation\n",
        "x_train_tfidf = []\n",
        "x_train_word2vec = np.empty((0,300), float)\n",
        "for item in x_train:\n",
        "  xtmp = sentence2vec(item)\n",
        "  x_train_word2vec = np.append(x_train_word2vec,xtmp[np.newaxis],0)\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=preprocessing, ngram_range=(1,1))\n",
        "vectorizer_model = vectorizer.fit(x_train) # This model will be applied to test data\n",
        "x_train_tfidf = vectorizer_model.transform(x_train).todense()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd8OaMYo8kV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3: prepare test dataset\n",
        "x_test_tfidf = []\n",
        "x_test_word2vec = np.empty((0,300), float)\n",
        "for item in x_test:\n",
        "  xtmp = sentence2vec(item)\n",
        "  x_test_word2vec = np.append(x_test_word2vec,xtmp[np.newaxis],0)\n",
        "\n",
        "x_test_tfidf = vectorizer_model.transform(x_test).todense()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y6KoZG49PtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 4: Build a SVM classifier and a kNN classifier\n",
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [2**i for i in [-10,-8,-6,-4,-2,0,2,4,6,8,10]],\n",
        "                     'C': [2**i for i in [-10,-8,-6,-4,-2,0,2,4,6,8,10]]},\n",
        "                    {'kernel': ['linear'], 'C': [2**i for i in [-10,-8,-6,-4,-2,0,2,4,6,8,10]]}]\n",
        "\n",
        "svc_model = GridSearchCV(SVC(), tuned_parameters, cv=5)\n",
        "svc_model.fit(x_train_tfidf, y_train)\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_model.fit(x_train_tfidf, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbxVsEgA-RbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 5: evaluate the classifiers\n",
        "y_predict_svm = svc_model.predict(x_test_tfidf)\n",
        "y_predict_knn = svc_model.predict(x_test_tfidf)\n",
        "\n",
        "print(\"Classification Performance for SVM\\n\")\n",
        "print(classification_report(y_test, y_predict_svm))\n",
        "print(confusion_matrix(y_test, y_predict_svm))\n",
        "\n",
        "print(\"\\nClassification Performance for kNN\\n\")\n",
        "print(classification_report(y_test, y_predict_knn))\n",
        "print(confusion_matrix(y_test, y_predict_knn))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}